{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mplfinance as mpf\n",
    "import scipy\n",
    "import yfinance as yf\n",
    "\n",
    "from neurotrader.download_data import download_df\n",
    "from neurotrader.directional_change import directional_change\n",
    "from neurotrader.perceptually_important import find_pips\n",
    "from neurotrader.head_shoulders import find_hs_patterns, get_pattern_return, load_attributes, HSPattern"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3a08e8ff7897739",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reversal Pattern Detection Algorithm\n",
    "- Look at the 1 day intervals for now\n",
    "- Want to look at min by min at some point\n",
    "- Goal: If a pattern is detected, how much wieght should we give the buying/selling power of that pattern\n",
    "- AKA HOW ACCURATE THAT PATTERN IS FOR a certain stock (Profit factor)\n",
    "- See which patterns are most common\n",
    "- Look for a way to add an \"Early Detection\" of a pattern\n",
    "\n",
    "\n",
    "## How to apply to a neural network short term\n",
    "- Build an CNN that classifies images of patterns.\n",
    "\n",
    "### How to apply to a neural network long term:\n",
    "Train a NN to detect different patterns, testing different exit strategies on each pattern. Have it assign a buy/sell confidence value to these patterns. Then test this on unseen new data. Try to impliment flag detection here too\n",
    "\n",
    "We can further test this by training the network on each stock individually, trying to make predictions off of early identification"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b09f16610e9058de"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building an ML model to detect patterns in a rolling window.\n",
    "Abstract: We want to use mathematical equations to detect patterns in our stock data in our data. We will look at those patterns and tie a value to them. We will then use a rolling window technique to make calculations on unseen data???\n",
    "- Use a 72-hour trend rolling window\n",
    "- LOOK FOR FEATURES that could show a trend\n",
    "- The Head and Shoulders is not a common occurance. Look to suplement this data with other patterns?\n",
    "- A lot of papers and programs create a bunch of synthetic data to train models on\n",
    "- This might be the best option here."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "192f1d2efb62c753"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Goal Here\n",
    "1) Test to see how the model when trained on hourly, minute, and daily data\n",
    "2) Train the data on both synthetic and real data. See how that effects pattern recognition\n",
    "    - NEED TO LEARN HOW TO GENERATE SYNTHETIC STOCK DATA\n",
    "3) Investigate the approach given by the paper from Seoul National University\n",
    "    - This may not be useful because they use a ton of different patterns. Interesting read about what type of model to use\n",
    "4) Have Neural Network add a buy/sell weight to each pattern after seeing if the stock went up or down after. Need to add manually?\n",
    "\n",
    "\n",
    "BREAK DATA INTO TRAINING AND TEST DATA FOR REAL DATA\n",
    "FOR SYNTHETIC DATA, TEST ON SAME TESTING DATA AS OTHER MODEL."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b0c74c1a9dd733e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1: Generating Head and Shoulders Patterns\n",
    "This code focuses on capturing time periods of code instead of images of code. Graphs can be generated as needed however. There is a possible way to do this automatically"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7705a4e480916549"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Read and print the stock tickers that make up S&P500\n",
    "tickers = pd.read_html(\n",
    "    'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]\n",
    "\n",
    "# Get the data for these tickers from yahoo finance\n",
    "# yf_top500 = yf.download(tickers.Symbol.to_list(), interval='60m', period='2y', auto_adjust=True)['Close']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bffa37ff3b6ee7cf",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Get the data for these tickers from yahoo finance\n",
    "# Downloading this data because it is very time-consuming and I don't want to make \n",
    "#   repeated calls to yfinance\n",
    "# COMMENTING OUT TO AVOID DOING THIS AGAIN\n",
    "# for ticker in tickers.Symbol.to_list():\n",
    "#     df = download_df(ticker=ticker, period='2y', interval='60m')\n",
    "#     ticker_name = 'data/' + ticker + '_data.csv'\n",
    "#     df.to_csv(ticker_name, index=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac4a6cf62014a8ec"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create a dictionary to store data\n",
    "# Use ticker names as key\n",
    "l_yf_top500_norm = {}\n",
    "# For each ticker\n",
    "for ticker in tickers.Symbol.to_list():\n",
    "    try:\n",
    "        # Get the data from the local files and apply logarithmic transformations to it\n",
    "        data = pd.read_csv('data/' + ticker + '_data.csv', index_col=None, header=0)\n",
    "        # Place the datetime in a temp dataset\n",
    "        temp = data['Datetime']\n",
    "        # Apply numeric and log to the data\n",
    "        data = data.apply(pd.to_numeric, errors='coerce')\n",
    "        data = np.log(data)\n",
    "        # Add the datetime we saved earlier back in\n",
    "        data['Datetime'] = temp\n",
    "        data.index = pd.DatetimeIndex(data['Datetime'])\n",
    "        # Get the close data and find the HS and IHS patterns\n",
    "        dat_slice = data['close'].to_numpy()\n",
    "        hs_patterns, ihs_patterns = find_hs_patterns(dat_slice, 6, early_find=False)\n",
    "        # Store the data, hs, IHS with the ticker name as key\n",
    "        l_yf_top500_norm.update({ticker: [data, hs_patterns, ihs_patterns]})\n",
    "    except Exception as e:\n",
    "        print(f\"Failed on ticker {ticker} with exception {e}\")\n",
    "        continue"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca578520c1bce2ee",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for i in l_yf_top500_norm:\n",
    "    print(f\"For ticker {i} there are {len(l_yf_top500_norm[i][1])} hs patterns and \"\n",
    "              f\"{len(l_yf_top500_norm[i][2])} inverted hs patterns\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92eb24602b154cd0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def plot_simple_hs(candle_data: pd.DataFrame, ticker: str, pat: HSPattern, pad: int = 2):\n",
    "    if pad < 0:\n",
    "        pad = 0\n",
    "    \n",
    "    data = candle_data.iloc[pat.start_i:pat.break_i + 1 + pad]\n",
    "    fname = \"images/hs/\" + ticker + '_' + str(pat.start_i) + '_' + str(pat.break_i) + \".png\"\n",
    "    \n",
    "    mpf.plot(data, type='candle', axisoff=True, style = 'classic', savefig=dict(fname=fname, dpi=60))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e215aee794aaf099",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for ticker in l_yf_top500_norm:\n",
    "    num_hs = len(l_yf_top500_norm[ticker][1])\n",
    "    num_ihs = len(l_yf_top500_norm[ticker][2])\n",
    "    \n",
    "    print(f\"For ticker {ticker} there are {num_hs} hs patterns and \"\n",
    "              f\"{num_ihs} inverted hs patterns\")\n",
    "    \n",
    "    for i in range(num_hs):\n",
    "        candle_data = l_yf_top500_norm[ticker][0]\n",
    "        pat = l_yf_top500_norm[ticker][1][i]\n",
    "        plot_simple_hs(candle_data, ticker, pat, 2)\n",
    "        \n",
    "    for i in range(num_ihs):\n",
    "        candle_data = l_yf_top500_norm[ticker][0]\n",
    "        pat = l_yf_top500_norm[ticker][2][i]\n",
    "        plot_simple_hs(candle_data, ticker, pat, 2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3715c0974d07d738",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bd5792e6100cb832"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
