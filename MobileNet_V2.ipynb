{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T02:31:58.243221Z",
     "start_time": "2024-04-29T02:31:54.788430Z"
    }
   },
   "id": "79e2410f2bada8e8",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Convert images from pixals into data, then use pretrained model to accurately classify the stocks\n",
    "- Mathematical detection algorithms are at best 84% accurate. Goal is to \n",
    "- https://www.tensorflow.org/tutorials/images/transfer_learning\n",
    "- Make a requirements text file\n",
    "- We do not apply data augmentation because we already have patterns classified as up or down, and flipping them would be counter intuitive"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8e9cd6425650704"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MobileNet V2 Trained by Google"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7a0ee41a9636bc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (128, 128)\n",
    "SHUFFLE_BUFFER_SIZE = 1000"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T13:45:42.273445Z",
     "start_time": "2024-04-29T13:45:42.270942Z"
    }
   },
   "id": "fd4b2021336e99c8",
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Paths: For local machine\n",
    "train_dir = 'C:/Users/Nick/Documents/SchoolStuff/spring2024/machineLearning/final_project/tradingpatterns/stock_patterns/train'\n",
    "validation_dir = 'C:/Users/Nick/Documents/SchoolStuff/spring2024/machineLearning/final_project/tradingpatterns/stock_patterns/validation'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T13:45:42.585423Z",
     "start_time": "2024-04-29T13:45:42.582919Z"
    }
   },
   "id": "baceab8354a8b153",
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16000 files belonging to 8 classes.\n",
      "Found 16000 files belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create training and validation dataset\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(train_dir,\n",
    "                                                            shuffle=True,\n",
    "                                                            batch_size=BATCH_SIZE,\n",
    "                                                            image_size=IMG_SIZE)\n",
    "\n",
    "validation_dataset = tf.keras.utils.image_dataset_from_directory(validation_dir,\n",
    "                                                                 shuffle=True,\n",
    "                                                                 batch_size=BATCH_SIZE,\n",
    "                                                                 image_size=IMG_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T13:45:45.004128Z",
     "start_time": "2024-04-29T13:45:42.853834Z"
    }
   },
   "id": "791c86a7422ff6d6",
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of validation batches: 400\n",
      "Number of test batches: 100\n"
     ]
    }
   ],
   "source": [
    "# Create a test set\n",
    "val_batches = tf.data.experimental.cardinality(validation_dataset)\n",
    "test_dataset = validation_dataset.take(val_batches // 5)\n",
    "validation_dataset = validation_dataset.skip(val_batches // 5)\n",
    "\n",
    "# Rescale the images from [-1 to 1] vs [0 to 255]\n",
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "\n",
    "print('Number of validation batches: %d' % tf.data.experimental.cardinality(validation_dataset))\n",
    "print('Number of test batches: %d' % tf.data.experimental.cardinality(test_dataset))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T13:45:45.011639Z",
     "start_time": "2024-04-29T13:45:45.005129Z"
    }
   },
   "id": "5fd4c100fb0d908f",
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T13:45:45.444302Z",
     "start_time": "2024-04-29T13:45:45.012641Z"
    }
   },
   "id": "39113a024e5db3df",
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# This feature extractor converts each 255x255 image into a 5x5x1280 block of features\n",
    "image_batch, label_batch = next(iter(train_dataset))\n",
    "feature_batch = base_model(image_batch)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T13:45:45.755278Z",
     "start_time": "2024-04-29T13:45:45.446806Z"
    }
   },
   "id": "fa0e05434194b03e",
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Unfreeze this to train\n",
    "base_model.trainable = False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T13:45:45.760786Z",
     "start_time": "2024-04-29T13:45:45.756280Z"
    }
   },
   "id": "e1fdb33f4fcb9884",
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1280)\n",
      "(32, 1)\n"
     ]
    }
   ],
   "source": [
    "# Adding a classification head\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "feature_batch_average = global_average_layer(feature_batch)\n",
    "print(feature_batch_average.shape)\n",
    "\n",
    "# Add a dense layer to convert it to a single prediction per image\n",
    "# Test relu, leaky_relu, and sigmoid\n",
    "prediction_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "prediction_batch = prediction_layer(feature_batch_average)\n",
    "print(prediction_batch.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T14:33:23.377145Z",
     "start_time": "2024-04-29T14:33:23.366629Z"
    }
   },
   "id": "8896967f2a65badb",
   "execution_count": 106
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(128, 128, 3))\n",
    "x = preprocess_input(inputs)\n",
    "x = base_model(x, training=False)\n",
    "x = global_average_layer(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = prediction_layer(x)\n",
    "model = tf.keras.Model(inputs, outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T14:33:23.702143Z",
     "start_time": "2024-04-29T14:33:23.690625Z"
    }
   },
   "id": "3a9fd423056ccbaf",
   "execution_count": 107
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), # from_logits=True\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy(threshold=0.5, name='accuracy')])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T14:34:59.055691Z",
     "start_time": "2024-04-29T14:34:59.048680Z"
    }
   },
   "id": "8676bd207c92643c",
   "execution_count": 110
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m400/400\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m46s\u001B[0m 110ms/step - accuracy: 0.1245 - loss: 1.1496\n"
     ]
    }
   ],
   "source": [
    "initial_epochs = 10\n",
    "\n",
    "loss0, accuracy0 = model.evaluate(validation_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T14:35:45.243434Z",
     "start_time": "2024-04-29T14:34:59.591512Z"
    }
   },
   "id": "a7f9d2da7571e2e6",
   "execution_count": 111
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loss: 0.54\n",
      "initial accuracy: 0.12\n"
     ]
    }
   ],
   "source": [
    "print(\"initial loss: {:.2f}\".format(loss0))\n",
    "print(\"initial accuracy: {:.2f}\".format(accuracy0))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T13:46:45.055105Z",
     "start_time": "2024-04-29T13:46:45.052100Z"
    }
   },
   "id": "96106009a76ef83a",
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001B[1m5226/5226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m474s\u001B[0m 90ms/step - binary_accuracy: 0.1139 - loss: -177.5586 - val_binary_accuracy: 0.1143 - val_loss: -690.9203\n",
      "Epoch 2/10\n",
      "\u001B[1m5226/5226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m470s\u001B[0m 90ms/step - binary_accuracy: 0.1138 - loss: -869.1248 - val_binary_accuracy: 0.1143 - val_loss: -1376.6044\n",
      "Epoch 3/10\n",
      "\u001B[1m5226/5226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m499s\u001B[0m 96ms/step - binary_accuracy: 0.1138 - loss: -1561.3773 - val_binary_accuracy: 0.1143 - val_loss: -2062.6399\n",
      "Epoch 4/10\n",
      "\u001B[1m5226/5226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m492s\u001B[0m 94ms/step - binary_accuracy: 0.1138 - loss: -2254.1765 - val_binary_accuracy: 0.1143 - val_loss: -2747.0378\n",
      "Epoch 5/10\n",
      "\u001B[1m5226/5226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m483s\u001B[0m 92ms/step - binary_accuracy: 0.1138 - loss: -2944.3350 - val_binary_accuracy: 0.1143 - val_loss: -3432.6667\n",
      "Epoch 6/10\n",
      "\u001B[1m5226/5226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m470s\u001B[0m 90ms/step - binary_accuracy: 0.1138 - loss: -3636.3640 - val_binary_accuracy: 0.1143 - val_loss: -4117.5293\n",
      "Epoch 7/10\n",
      "\u001B[1m5226/5226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m442s\u001B[0m 85ms/step - binary_accuracy: 0.1138 - loss: -4329.1030 - val_binary_accuracy: 0.1143 - val_loss: -4805.3682\n",
      "Epoch 8/10\n",
      "\u001B[1m5226/5226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m442s\u001B[0m 85ms/step - binary_accuracy: 0.1138 - loss: -5017.1211 - val_binary_accuracy: 0.1143 - val_loss: -5490.5518\n",
      "Epoch 9/10\n",
      "\u001B[1m5226/5226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m453s\u001B[0m 87ms/step - binary_accuracy: 0.1138 - loss: -5712.8535 - val_binary_accuracy: 0.1143 - val_loss: -6172.8169\n",
      "Epoch 10/10\n",
      "\u001B[1m5226/5226\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m448s\u001B[0m 86ms/step - binary_accuracy: 0.1138 - loss: -6402.2705 - val_binary_accuracy: 0.1143 - val_loss: -6860.9946\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset,\n",
    "                    epochs=initial_epochs,\n",
    "                    validation_data=validation_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T03:52:04.707025Z",
     "start_time": "2024-04-29T02:34:10.494283Z"
    }
   },
   "id": "13c2641425b961f8",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'val_accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m acc \u001B[38;5;241m=\u001B[39m history\u001B[38;5;241m.\u001B[39mhistory[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbinary_accuracy\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m----> 2\u001B[0m val_acc \u001B[38;5;241m=\u001B[39m \u001B[43mhistory\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhistory\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mval_accuracy\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[0;32m      4\u001B[0m loss \u001B[38;5;241m=\u001B[39m history\u001B[38;5;241m.\u001B[39mhistory[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m      5\u001B[0m val_loss \u001B[38;5;241m=\u001B[39m history\u001B[38;5;241m.\u001B[39mhistory[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_loss\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "\u001B[1;31mKeyError\u001B[0m: 'val_accuracy'"
     ]
    }
   ],
   "source": [
    "acc = history.history['binary_accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T03:52:05.725585Z",
     "start_time": "2024-04-29T03:52:04.708528Z"
    }
   },
   "id": "8492f7380af3b1a3",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "base_model.trainable = True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T03:52:05.726587Z",
     "start_time": "2024-04-29T03:52:05.726587Z"
    }
   },
   "id": "9464e981f00f2d5f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = 100\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "  layer.trainable = False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T03:52:05.727589Z",
     "start_time": "2024-04-29T03:52:05.727589Z"
    }
   },
   "id": "f1fcb09b703aa5a1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              optimizer = tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate/10),\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy(threshold=0.5, name='binary_accuracy')])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fd6c9334ff2d9dd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "fine_tune_epochs = 10\n",
    "total_epochs =  initial_epochs + fine_tune_epochs\n",
    "\n",
    "history_fine = model.fit(train_dataset,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch=len(history.epoch),\n",
    "                         validation_data=validation_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-29T03:52:05.729091Z"
    }
   },
   "id": "a1fcdf30b27e330d",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
