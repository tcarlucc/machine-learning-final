{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79e2410f2bada8e8",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T23:52:16.262656Z",
     "start_time": "2024-05-02T23:52:12.311103Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorboard\n",
    "\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e9cd6425650704",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Convert images from pixals into data, then use pretrained model to accurately classify the stocks\n",
    "- Mathematical detection algorithms are at best 84% accurate. Goal is to \n",
    "- https://www.tensorflow.org/tutorials/images/transfer_learning\n",
    "- Make a requirements text file\n",
    "- We do not apply data augmentation because we already have patterns classified as up or down, and flipping them would be counterintuitive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a0ee41a9636bc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### MobileNet V2 Trained by Google"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# %reload_ext tensorboard"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T23:52:16.266662Z",
     "start_time": "2024-05-02T23:52:16.264159Z"
    }
   },
   "id": "27ecc2b2347db0ed",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2335e19cbdd38f9b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T23:52:16.278180Z",
     "start_time": "2024-05-02T23:52:16.267664Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension.\n",
    "# %load_ext tensorboard\n",
    "# \n",
    "# from tensorflow import keras\n",
    "# print(\"TensorFlow version: \", tf.__version__)\n",
    "# assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "#     \"This notebook requires TensorFlow 2.0 or above.\"\n",
    "# print(tensorboard.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd4b2021336e99c8",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T23:52:16.283188Z",
     "start_time": "2024-05-02T23:52:16.279182Z"
    }
   },
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baceab8354a8b153",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T23:52:16.291200Z",
     "start_time": "2024-05-02T23:52:16.284189Z"
    }
   },
   "outputs": [],
   "source": [
    "# Paths: For local machine\n",
    "train_dir = 'C:/Users/Nick/Documents/SchoolStuff/spring2024/machineLearning/final_project/CNN/tradingpatterns/stock_patterns/train'\n",
    "validation_dir = 'C:/Users/Nick/Documents/SchoolStuff/spring2024/machineLearning/final_project/CNN/tradingpatterns/stock_patterns/validation'\n",
    "\n",
    "# train_dir = 'C:/Users/Nick/Desktop/stock images/train'\n",
    "# validation_dir = 'C:/Users/Nick/Desktop/stock images/validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "791c86a7422ff6d6",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T23:52:16.661768Z",
     "start_time": "2024-05-02T23:52:16.292202Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 files belonging to 4 classes.\n",
      "Found 2000 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create training and validation dataset\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(train_dir,\n",
    "                                                            shuffle=True,\n",
    "                                                            batch_size=BATCH_SIZE,\n",
    "                                                            color_mode='rgb',\n",
    "                                                            image_size=(IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "validation_dataset = tf.keras.utils.image_dataset_from_directory(validation_dir,\n",
    "                                                                 shuffle=True,\n",
    "                                                                 batch_size=BATCH_SIZE,\n",
    "                                                                 color_mode='rgb',\n",
    "                                                                 image_size=(IMG_SIZE, IMG_SIZE))\n",
    "class_names = train_dataset.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fd4c100fb0d908f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T23:52:16.674287Z",
     "start_time": "2024-05-02T23:52:16.662769Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of validation batches: 51\n",
      "Number of test batches: 12\n"
     ]
    }
   ],
   "source": [
    "# Create a test set\n",
    "val_batches = tf.data.experimental.cardinality(validation_dataset)\n",
    "test_dataset = validation_dataset.take(val_batches // 5)\n",
    "validation_dataset = validation_dataset.skip(val_batches // 5)\n",
    "\n",
    "# Rescale the images from [-1 to 1] vs [0 to 255]\n",
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "\n",
    "print('Number of validation batches: %d' % tf.data.experimental.cardinality(validation_dataset))\n",
    "print('Number of test batches: %d' % tf.data.experimental.cardinality(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39113a024e5db3df",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T23:52:17.290230Z",
     "start_time": "2024-05-02T23:52:16.674788Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE) + (3,)\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa0e05434194b03e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T23:52:18.129015Z",
     "start_time": "2024-05-02T23:52:17.291232Z"
    }
   },
   "outputs": [],
   "source": [
    "# This feature extractor converts each 255x255 image into a 5x5x1280 block of features\n",
    "image_batch, label_batch = next(iter(train_dataset))\n",
    "feature_batch = base_model(image_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1fdb33f4fcb9884",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T23:52:18.170579Z",
     "start_time": "2024-05-02T23:52:18.130518Z"
    }
   },
   "outputs": [],
   "source": [
    "# Unfreeze this to train\n",
    "base_model.trainable = False\n",
    "\n",
    "# Adding a classification head using max\n",
    "global_max_layer = tf.keras.layers.GlobalMaxPooling2D()\n",
    "feature_batch_max = global_max_layer(feature_batch)\n",
    "# print(feature_batch_average.shape)\n",
    "\n",
    "# Add a dense layer to convert it to a single prediction per image , activation='softmax'\n",
    "prediction_layer = tf.keras.layers.Dense(len(class_names)) \n",
    "prediction_batch = prediction_layer(feature_batch_max)\n",
    "# print(prediction_batch.shape)\n",
    "\n",
    "# Process the inputs\n",
    "inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "# Add the preprocessing layer\n",
    "x = preprocess_input(inputs)\n",
    "# Add the base model after, keeping training to false\n",
    "x = base_model(x, training=False)\n",
    "# Add max pooling layer\n",
    "x = global_max_layer(x)\n",
    "# Add dropout layer\n",
    "# x = tf.keras.layers.Dropout(0)(x)\n",
    "# Add softmax prediction layer\n",
    "outputs = prediction_layer(x)\n",
    "# Create a model from inputs, outputs.\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cd8604d15ceed5e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T23:52:18.768495Z",
     "start_time": "2024-05-02T23:52:18.749466Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "\u001B[1mModel: \"functional_1\"\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_1 (\u001B[38;5;33mInputLayer\u001B[0m)      │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m224\u001B[0m, \u001B[38;5;34m224\u001B[0m, \u001B[38;5;34m3\u001B[0m)    │             \u001B[38;5;34m0\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ true_divide (\u001B[38;5;33mTrueDivide\u001B[0m)        │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m224\u001B[0m, \u001B[38;5;34m224\u001B[0m, \u001B[38;5;34m3\u001B[0m)    │             \u001B[38;5;34m0\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ subtract (\u001B[38;5;33mSubtract\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m224\u001B[0m, \u001B[38;5;34m224\u001B[0m, \u001B[38;5;34m3\u001B[0m)    │             \u001B[38;5;34m0\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ mobilenetv2_1.00_224            │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m7\u001B[0m, \u001B[38;5;34m7\u001B[0m, \u001B[38;5;34m1280\u001B[0m)     │     \u001B[38;5;34m2,257,984\u001B[0m │\n│ (\u001B[38;5;33mFunctional\u001B[0m)                    │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_max_pooling2d            │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1280\u001B[0m)           │             \u001B[38;5;34m0\u001B[0m │\n│ (\u001B[38;5;33mGlobalMaxPooling2D\u001B[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (\u001B[38;5;33mDense\u001B[0m)                   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m4\u001B[0m)              │         \u001B[38;5;34m5,124\u001B[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ true_divide (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TrueDivide</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ subtract (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Subtract</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_max_pooling2d            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,124</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m2,263,108\u001B[0m (8.63 MB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,263,108</span> (8.63 MB)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m5,124\u001B[0m (20.02 KB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,124</span> (20.02 KB)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m2,257,984\u001B[0m (8.61 MB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72c75cb67a26d13",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8676bd207c92643c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), # False when we have a softmax layer\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f9d2da7571e2e6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logdir = os.path.join(\"C:/Users/Nick/Documents/SchoolStuff/spring2024/machineLearning/final_project/CNN/logs\", datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c2641425b961f8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the top layer\n",
    "initial_epochs = 5\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=initial_epochs,\n",
    "                    steps_per_epoch=5,\n",
    "                    validation_data=validation_dataset,\n",
    "                    validation_steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca4ad6f-c9a8-4d94-b488-fb055a80ab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs --host localhost --port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8492f7380af3b1a3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "# \n",
    "# plt.figure(figsize=(8, 8))\n",
    "# plt.subplot(2, 1, 1)\n",
    "# plt.plot(acc, label='Training Accuracy')\n",
    "# plt.plot(val_acc, label='Validation Accuracy')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.ylim([0, .5])\n",
    "# plt.title('Training and Validation Accuracy')\n",
    "# \n",
    "# plt.subplot(2, 1, 2)\n",
    "# plt.plot(loss, label='Training Loss')\n",
    "# plt.plot(val_loss, label='Validation Loss')\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.ylabel('Cross Entropy')\n",
    "# plt.title('Training and Validation Loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylim([0, 10])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44995519b11c85d9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Customization\n",
    "1) Feature Extraction\n",
    "2) Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9464e981f00f2d5f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fcb09b703aa5a1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = 130\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "  layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69df5e4b48494ed6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd6c9334ff2d9dd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate/10),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), # False when we have a Softmax layer\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fcdf30b27e330d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fine_tune_epochs = 5\n",
    "total_epochs =  initial_epochs + fine_tune_epochs\n",
    "\n",
    "history_fine = model.fit(train_dataset,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch=len(history.epoch),\n",
    "                         steps_per_epoch=5,\n",
    "                         validation_data=validation_dataset,\n",
    "                         validation_steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c38e6490bb84be",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
    "print('Test accuracy :', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108ab22ee0bbe9b2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc += history_fine.history['accuracy']\n",
    "val_acc += history_fine.history['val_accuracy']\n",
    "\n",
    "loss += history_fine.history['loss']\n",
    "val_loss += history_fine.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0193ea8c91d943",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.ylim([0, .5])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "          plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.ylim([0, 8.0])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "         plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7b517250283fea",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add a softmax layer to our model and apply our test data\n",
    "probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])\n",
    "image_batch, label_batch = test_dataset.as_numpy_iterator().next()\n",
    "predictions = probability_model.predict(image_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a7ceb6d6efd1ee",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4daaa4ff9673eac",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(9):\n",
    "  ax = plt.subplot(3, 3, i + 1)\n",
    "  plt.imshow(image_batch[i].astype(\"uint8\"))\n",
    "  guess = str(class_names[np.argmax(predictions[i+9])])\n",
    "  actual = str(class_names[label_batch[i+9]])\n",
    "  title = \"Prediction: \" + guess + \"\\n\" + \"Actual: \" + actual\n",
    "  plt.title(title)\n",
    "  plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352bdbddb8403c1e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %tensorboard --logdir logs --host localhost --port 6006"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
